{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \n",
      "[GCC 7.3.0]\n",
      "['', '/home/adam/Developer/kalibr_workspace/devel/lib/python2.7/dist-packages', '/opt/ros/kinetic/lib/python2.7/dist-packages', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python36.zip', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/lib-dynload', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/IPython/extensions', '/home/adam/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "print(sys.version)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.remove('/home/adam/Developer/kalibr_workspace/devel/lib/python2.7/dist-packages')\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import gzip\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from six.moves import xrange\n",
    "from urllib.request import urlretrieve\n",
    "from scipy.misc import imsave, imread, imresize\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment below two lines to implement this code with GPU\n",
    "#os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "DIR_PATH = os.path.dirname(os.path.realpath('/home/adam/sig_proc/'))\n",
    "DATA_DIR = os.path.join(DIR_PATH, \"data\")  # path for your results\n",
    "NUM_IMAGES = 60000\n",
    "DATABASE_NAME = 'mnist'\n",
    "OUTPUT_NAME = 'out'\n",
    "OUTPUT_PATH = os.path.join(DATA_DIR, OUTPUT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adam/data/out'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is to implement DCGAN. You need to complete TODO parts by yourself according to DCGAN topology\n",
    "class DCGAN():\n",
    "    def __init__(self, sess, img_size, z_dim, batch_size, epoch):\n",
    "        self.sess = sess\n",
    "        self.epoch = epoch\n",
    "        self.z_dim = z_dim\n",
    "        self.img_size = img_size\n",
    "        self.img_dim = img_size * img_size\n",
    "        self.img_shape = [img_size, img_size, 1]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.build_model()\n",
    "        self.model_name = \"DCGAN.model\"\n",
    "\n",
    "    def build_model(self):\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        self.img = tf.placeholder(tf.float32, [None]+self.img_shape, name='real_images')\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n",
    "        self.z_sum = tf.summary.histogram('z', self.z)\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        # self.img_fake is produced by generator with a random input z.\n",
    "        self.img_fake = self.generator(self.z)\n",
    "\n",
    "        # Outputs from discriminator with real image or fake image.\n",
    "        self.D, self.D_logits_real = self.discriminator(self.img)\n",
    "        self.D_fake, self.D_logits_fake = self.discriminator(self.img_fake, reuse=True)\n",
    "\n",
    "        self.d_sum = tf.summary.histogram(\"d\", self.D)\n",
    "        self.d__sum = tf.summary.histogram(\"d_\", self.D_fake)\n",
    "        self.img_fake_sum = tf.summary.histogram(\"G\", self.img_fake)\n",
    "\n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "        self.g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "        # Calculating Loss value\n",
    "        self.D_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_real, labels=tf.ones_like(self.D_logits_real)))\n",
    "        self.D_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_fake, labels=tf.zeros_like(self.D_logits_fake)))\n",
    "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
    "        self.G_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_fake, labels=tf.ones_like(self.D_logits_fake)))\n",
    "\n",
    "\n",
    "    def generator(self, z, is_training=True, reuse=False):\n",
    "        # TODO: generate fake image with randonm z with 4 layers\n",
    "        alpha = 0.2\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "\n",
    "            # First fully connected layer\n",
    "            x1 = tf.layers.dense(z, 7*7*256)\n",
    "            x1 = tf.reshape(x1, (-1, 7, 7, 256))\n",
    "            x1 = tf.layers.batch_normalization(x1, training=is_training)\n",
    "            x1 = tf.maximum(alpha * x1, x1) # works as relu\n",
    "\n",
    "            # Second convolution layer\n",
    "            # TODO: After 2d transpose convolution, the shape of x2 is 14*14*128\n",
    "            x2 = tf.layers.conv2d_transpose(x1, 128, 5, strides=2, padding='SAME')\n",
    "            # TODO: Batch normalization of x2\n",
    "            x2 = tf.layers.batch_normalization(x2, training=is_training)\n",
    "\n",
    "            x2 = tf.maximum(alpha * x2, x2)\n",
    "\n",
    "            # Third convolution layer\n",
    "            # TODO:2d transpose convolution as the second convolution layer, after that, the shape of x3 is 28*28*64\n",
    "            x3 = tf.layers.conv2d_transpose(x2, 64, 5, strides=2, padding='SAME')\n",
    "            # Batch normalization of x3\n",
    "            x3 = tf.layers.batch_normalization(x3, training=is_training)\n",
    "            x3 = tf.maximum(alpha * x3, x3)\n",
    "\n",
    "            drop = tf.nn.dropout(x3, keep_prob=0.5)\n",
    "\n",
    "            # Output layer\n",
    "            logits = tf.layers.conv2d_transpose(drop, 1, 5, strides=1, padding='same')\n",
    "\n",
    "            out = tf.tanh(logits)\n",
    "\n",
    "            return out\n",
    "\n",
    "    def discriminator(self, img, reuse=False):\n",
    "        # TODO: discriminate the input img whether a real one or fake one.\n",
    "        alpha = 0.2\n",
    "        with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "            # The shape of img is 28*28*3, the shape of x1 is 14*14*64\n",
    "            x1 = tf.layers.conv2d(img, 64, 5, strides=2, padding='same')\n",
    "            x1 = tf.maximum(alpha * x1, x1)\n",
    "\n",
    "            # TODO: Second convolution layer, the shape of x2 is 7*7*128\n",
    "            x2 = tf.layers.conv2d(x1, 128, 5, strides=2, padding='same')\n",
    "            # TODO: Batch normalization of x2\n",
    "            bn2 = tf.layers.batch_normalization(x2)\n",
    "            x2 = tf.maximum(alpha * bn2, bn2)\n",
    "\n",
    "            # TODO: complete third convolution layer, the shape of x3 should be 4*4*256\n",
    "            x3 = tf.layers.conv2d(img, 256, 5, strides=2, padding='same')\n",
    "            # batch_normalization\n",
    "            bn3 = tf.layers.batch_normalization(x3)\n",
    "            x3 = tf.maximum(alpha * bn3, bn3)\n",
    "\n",
    "            # last layer\n",
    "            x4 = tf.reshape(x3, (-1, 4*4*256))\n",
    "            logits = tf.layers.dense(x4, 1)\n",
    "            out = tf.sigmoid(logits)\n",
    "            return out, logits\n",
    "\n",
    "    def plot(self, samples):\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(self.img_size, self.img_size), cmap=\"Greys_r\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def train(self, output_path):\n",
    "        d_optim = tf.train.AdamOptimizer().minimize(self.D_loss, var_list=self.d_vars)\n",
    "        g_optim = tf.train.AdamOptimizer().minimize(self.G_loss, var_list=self.g_vars)\n",
    "\n",
    "        try:\n",
    "            tf.global_variables_initializer().run()\n",
    "        except:\n",
    "            tf.initialize_all_variables().run()\n",
    "\n",
    "        i = 0\n",
    "        counter = 1\n",
    "        mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)\n",
    "\n",
    "        for epoch in xrange(self.epoch):\n",
    "            input_z = np.random.uniform(-1, 1, [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "            input_imgs, _ = mnist.train.next_batch(self.batch_size)\n",
    "            input_imgs_ = input_imgs.reshape((self.batch_size, self.img_size, self.img_size, 1))\n",
    "            # Update D network\n",
    "            _, D_loss_curr = self.sess.run([d_optim, self.D_loss], feed_dict={self.img: input_imgs_, self.z: input_z})\n",
    "            _, G_loss_curr = self.sess.run([g_optim, self.G_loss], feed_dict={self.z: input_z})\n",
    "\n",
    "            t_vars = tf.trainable_variables()\n",
    "            d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "            g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "            counter += 1\n",
    "            if counter % 1000 == 0 or counter == 1:\n",
    "                print(\"Epoch: [{:2d}] D_loss: {:.8f}, G_loss {:.8f}\".format(\n",
    "                    epoch, D_loss_curr, G_loss_curr))\n",
    "\n",
    "                samples = self.sess.run(self.img_fake, feed_dict={self.z: input_z})\n",
    "                #import pdb; pdb.set_trace()\n",
    "                fig = self.plot(samples)\n",
    "                if not os.path.exists(output_path):\n",
    "                    os.makedirs(output_path)\n",
    "                plt.savefig(os.path.join(output_path, '{}.png'.format(str(i).zfill(3))), bbox_inches='tight')\n",
    "                i += 1\n",
    "                plt.close(fig)\n",
    "def run():\n",
    "\n",
    "    config = tf.ConfigProto(\n",
    "            device_count = {'GPU': 0}) # If you wanna implement this code with GPU, change 0 to 1 or 2\n",
    "    with tf.Session(config=config) as sess:\n",
    "        dcgan = DCGAN(sess, img_size=28, z_dim=100, batch_size=16, epoch=100000)\n",
    "        dcgan.train(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-2753312d71eb>:136: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: [998] D_loss: 0.88743114, G_loss 1.17663145\n",
      "Epoch: [1998] D_loss: 0.83994001, G_loss 1.29918873\n",
      "Epoch: [2998] D_loss: 0.95725387, G_loss 1.00781584\n",
      "Epoch: [3998] D_loss: 1.02833200, G_loss 1.13382924\n",
      "Epoch: [4998] D_loss: 0.86837924, G_loss 1.60037327\n",
      "Epoch: [5998] D_loss: 0.92037106, G_loss 1.20377076\n",
      "Epoch: [6998] D_loss: 0.81362414, G_loss 1.35328889\n",
      "Epoch: [7998] D_loss: 1.06136620, G_loss 1.04219210\n",
      "Epoch: [8998] D_loss: 0.66852355, G_loss 1.55764806\n",
      "Epoch: [9998] D_loss: 0.82356536, G_loss 1.00095916\n",
      "Epoch: [10998] D_loss: 0.86486077, G_loss 1.33390868\n",
      "Epoch: [11998] D_loss: 0.84071028, G_loss 1.19391930\n",
      "Epoch: [12998] D_loss: 0.57648230, G_loss 1.68403006\n",
      "Epoch: [13998] D_loss: 0.73325366, G_loss 1.30106556\n",
      "Epoch: [14998] D_loss: 0.86292499, G_loss 1.13456047\n",
      "Epoch: [15998] D_loss: 0.80548012, G_loss 1.37758315\n",
      "Epoch: [16998] D_loss: 0.84063971, G_loss 1.34086919\n",
      "Epoch: [17998] D_loss: 0.68293720, G_loss 1.86828303\n",
      "Epoch: [18998] D_loss: 0.69834042, G_loss 1.30850625\n",
      "Epoch: [19998] D_loss: 0.79419518, G_loss 1.66482544\n",
      "Epoch: [20998] D_loss: 0.78694063, G_loss 1.68358803\n",
      "Epoch: [21998] D_loss: 0.83873332, G_loss 1.44472742\n",
      "Epoch: [22998] D_loss: 0.79272938, G_loss 0.98961359\n",
      "Epoch: [23998] D_loss: 0.84728324, G_loss 1.39957583\n",
      "Epoch: [24998] D_loss: 0.70789027, G_loss 1.46759033\n",
      "Epoch: [25998] D_loss: 0.76377708, G_loss 1.88870895\n",
      "Epoch: [26998] D_loss: 0.82914078, G_loss 1.74250042\n",
      "Epoch: [27998] D_loss: 0.78959537, G_loss 0.93761146\n",
      "Epoch: [28998] D_loss: 0.72474450, G_loss 1.62088311\n",
      "Epoch: [29998] D_loss: 0.91445971, G_loss 0.92956287\n",
      "Epoch: [30998] D_loss: 0.63074702, G_loss 1.86178303\n",
      "Epoch: [31998] D_loss: 0.59901601, G_loss 1.23953795\n",
      "Epoch: [32998] D_loss: 0.82592422, G_loss 1.54805112\n",
      "Epoch: [33998] D_loss: 0.65478021, G_loss 1.62422645\n",
      "Epoch: [34998] D_loss: 0.73328090, G_loss 1.75196779\n",
      "Epoch: [35998] D_loss: 0.81920505, G_loss 1.33923686\n",
      "Epoch: [36998] D_loss: 0.79525363, G_loss 1.15461981\n",
      "Epoch: [37998] D_loss: 0.57108325, G_loss 2.00866818\n",
      "Epoch: [38998] D_loss: 0.70697463, G_loss 1.95912623\n",
      "Epoch: [39998] D_loss: 0.67424434, G_loss 2.56437254\n",
      "Epoch: [40998] D_loss: 0.73829317, G_loss 1.87544847\n",
      "Epoch: [41998] D_loss: 0.62533015, G_loss 0.95990473\n",
      "Epoch: [42998] D_loss: 0.44342265, G_loss 3.03916717\n",
      "Epoch: [43998] D_loss: 0.55236226, G_loss 1.71832895\n",
      "Epoch: [44998] D_loss: 0.72651869, G_loss 1.88162422\n",
      "Epoch: [45998] D_loss: 0.67889106, G_loss 3.13991690\n",
      "Epoch: [46998] D_loss: 0.55266178, G_loss 3.30335188\n",
      "Epoch: [47998] D_loss: 0.88549066, G_loss 1.19952762\n",
      "Epoch: [48998] D_loss: 0.68351507, G_loss 1.75992751\n",
      "Epoch: [49998] D_loss: 0.83822054, G_loss 1.40514636\n",
      "Epoch: [50998] D_loss: 1.03327012, G_loss 1.57110405\n",
      "Epoch: [51998] D_loss: 0.65417534, G_loss 1.26731920\n",
      "Epoch: [52998] D_loss: 0.67285252, G_loss 1.32504487\n",
      "Epoch: [53998] D_loss: 0.81673610, G_loss 1.82441962\n",
      "Epoch: [54998] D_loss: 0.78531814, G_loss 1.82070506\n",
      "Epoch: [55998] D_loss: 0.56918561, G_loss 2.57921100\n",
      "Epoch: [56998] D_loss: 0.80917025, G_loss 1.14210165\n",
      "Epoch: [57998] D_loss: 0.64304936, G_loss 1.78527212\n",
      "Epoch: [58998] D_loss: 0.65862334, G_loss 2.71236944\n",
      "Epoch: [59998] D_loss: 0.72170663, G_loss 1.65833426\n",
      "Epoch: [60998] D_loss: 0.87248886, G_loss 1.20838296\n",
      "Epoch: [61998] D_loss: 0.62905681, G_loss 1.00526083\n",
      "Epoch: [62998] D_loss: 0.55446100, G_loss 2.84553003\n",
      "Epoch: [63998] D_loss: 0.84432876, G_loss 1.42460442\n",
      "Epoch: [64998] D_loss: 0.70397848, G_loss 1.94622123\n",
      "Epoch: [65998] D_loss: 0.57667160, G_loss 2.12276983\n",
      "Epoch: [66998] D_loss: 0.76245499, G_loss 0.84242421\n",
      "Epoch: [67998] D_loss: 0.77729464, G_loss 2.49375677\n",
      "Epoch: [68998] D_loss: 0.69203514, G_loss 1.18891406\n",
      "Epoch: [69998] D_loss: 0.50892878, G_loss 2.20026445\n",
      "Epoch: [70998] D_loss: 0.69935703, G_loss 1.94535208\n",
      "Epoch: [71998] D_loss: 0.77490371, G_loss 1.71185958\n",
      "Epoch: [72998] D_loss: 0.82229900, G_loss 1.93226635\n",
      "Epoch: [73998] D_loss: 0.82829744, G_loss 1.53748965\n",
      "Epoch: [74998] D_loss: 0.58628201, G_loss 1.67444444\n",
      "Epoch: [75998] D_loss: 0.59090459, G_loss 1.63408148\n",
      "Epoch: [76998] D_loss: 0.42032525, G_loss 2.36545086\n",
      "Epoch: [77998] D_loss: 0.66963929, G_loss 3.23887753\n",
      "Epoch: [78998] D_loss: 0.64529246, G_loss 1.65468764\n",
      "Epoch: [79998] D_loss: 0.44505712, G_loss 1.79465377\n",
      "Epoch: [80998] D_loss: 1.06047750, G_loss 1.18909562\n",
      "Epoch: [81998] D_loss: 0.51670617, G_loss 1.54669499\n",
      "Epoch: [82998] D_loss: 0.46759439, G_loss 2.77416515\n",
      "Epoch: [83998] D_loss: 0.76113069, G_loss 1.75176406\n",
      "Epoch: [84998] D_loss: 0.80989075, G_loss 2.34041333\n",
      "Epoch: [85998] D_loss: 0.42587757, G_loss 2.80021572\n",
      "Epoch: [86998] D_loss: 0.64390373, G_loss 1.69303739\n",
      "Epoch: [87998] D_loss: 0.84695911, G_loss 1.44826627\n",
      "Epoch: [88998] D_loss: 0.61267066, G_loss 1.33668375\n",
      "Epoch: [89998] D_loss: 0.68381321, G_loss 1.03648484\n",
      "Epoch: [90998] D_loss: 0.87940085, G_loss 1.63483381\n",
      "Epoch: [91998] D_loss: 0.72070104, G_loss 1.31089437\n",
      "Epoch: [92998] D_loss: 0.60901266, G_loss 1.48395395\n",
      "Epoch: [93998] D_loss: 0.64583468, G_loss 1.67954051\n",
      "Epoch: [94998] D_loss: 0.53783548, G_loss 2.52389240\n",
      "Epoch: [95998] D_loss: 0.62607908, G_loss 2.06007981\n",
      "Epoch: [96998] D_loss: 1.02303982, G_loss 2.00481701\n",
      "Epoch: [97998] D_loss: 0.65638858, G_loss 1.65155470\n",
      "Epoch: [98998] D_loss: 0.79914796, G_loss 1.77403355\n",
      "Epoch: [99998] D_loss: 0.49448949, G_loss 1.84156990\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
