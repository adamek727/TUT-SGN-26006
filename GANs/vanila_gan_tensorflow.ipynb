{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \n",
      "[GCC 7.3.0]\n",
      "['', '/home/adam/Developer/kalibr_workspace/devel/lib/python2.7/dist-packages', '/opt/ros/kinetic/lib/python2.7/dist-packages', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python36.zip', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/lib-dynload', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages', '/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/IPython/extensions', '/home/adam/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "print(sys.version)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.remove('/home/adam/Developer/kalibr_workspace/devel/lib/python2.7/dist-packages')\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import gzip\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from six.moves import xrange\n",
    "from urllib.request import urlretrieve\n",
    "from scipy.misc import imsave, imread, imresize\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment below two lines to implement this code with GPU\n",
    "os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "DIR_PATH = os.path.dirname(os.path.realpath('/home/adam/sig_proc/'))\n",
    "DATA_DIR = os.path.join(DIR_PATH, \"data\")  # path for your result\n",
    "\n",
    "NUM_IMAGES = 60000\n",
    "DATABASE_NAME = 'mnist'\n",
    "OUTPUT_NAME = 'out_vanilla'\n",
    "OUTPUT_PATH = os.path.join(DATA_DIR, OUTPUT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adam'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This section is to implement vanilla GAN. You need to complete TODO by yourself according to vanilla GAN theory.\n",
    "class VanillaGAN():\n",
    "    def __init__(self, sess, img_size, z_dim, hidden_dim, batch_size, epoch):\n",
    "        self.sess = sess\n",
    "        self.epoch = epoch\n",
    "        self.z_dim = z_dim\n",
    "        self.img_size = img_size\n",
    "        self.img_dim = img_size * img_size\n",
    "        self.img_shape = [img_size, img_size, 1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.build_model()\n",
    "        self.model_name = \"vanilla_GAN.model\"\n",
    "\n",
    "    def build_model(self):\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        self.img = tf.placeholder(tf.float32, [None, self.img_dim], name='real_images')\n",
    "\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n",
    "        self.z_sum = tf.summary.histogram('z', self.z)\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        # parameters for generator\n",
    "        # input layer parameters ()\n",
    "        self.G_W1 = tf.Variable(initializer([self.z_dim, self.hidden_dim]))\n",
    "        self.G_b1 = tf.Variable(tf.zeros(shape=[self.hidden_dim]))\n",
    "\n",
    "        # output layer parameters\n",
    "        self.G_W2 = tf.Variable(initializer([self.hidden_dim, self.img_dim]))\n",
    "        self.G_b2 = tf.Variable(tf.zeros(shape=[self.img_dim]))\n",
    "\n",
    "        # parameters for discriminator\n",
    "        # input layer parameters\n",
    "        self.D_W1 = tf.Variable(initializer([self.img_dim, self.hidden_dim]))\n",
    "        self.D_b1 = tf.Variable(tf.zeros(shape=[self.hidden_dim]))\n",
    "\n",
    "        self.D_W2 = tf.Variable(initializer([self.hidden_dim, 1]))\n",
    "        self.D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "        # self.img_fake is produced by generator with a random input z\n",
    "        self.img_fake = self.generator(self.z, self.G_W1, self.G_b1, self.G_W2, self.G_b2)\n",
    "\n",
    "        # self.D presents\n",
    "        # self.D_fake presents\n",
    "        self.D, self.D_logits_real = self.discriminator(self.img, self.D_W1, self.D_b1, self.D_W2, self.D_b2)\n",
    "        self.D_fake, self.D_logits_fake = self.discriminator(self.img_fake, self.D_W1, self.D_b1, self.D_W2, self.D_b2)\n",
    "\n",
    "        self.d_sum = tf.summary.histogram(\"d\", self.D)\n",
    "        self.d__sum = tf.summary.histogram(\"d_\", self.D_fake)\n",
    "        self.img_fake_sum = tf.summary.histogram(\"G\", self.img_fake)\n",
    "\n",
    "        self.D_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_real, labels=tf.ones_like(self.D_logits_real)))\n",
    "        self.D_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_fake, labels=tf.zeros_like(self.D_logits_fake)))\n",
    "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
    "        self.G_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_fake, labels=tf.ones_like(self.D_logits_fake)))\n",
    "\n",
    "        self.D_vars = [self.D_W1, self.D_b1, self.D_W2, self.D_b2]\n",
    "        self.G_vars = [self.G_W1, self.G_b1, self.G_W2, self.G_b2]\n",
    "\n",
    "    def generator(self, z, W1, b1, W2, b2):\n",
    "        # TODO: generate fake image with W1, b1, W2, b2\n",
    "        # Hints: using tf.nn.relu(), tf.matmul(), tf.nn.sigmoid() to implement functions as:\n",
    "        \n",
    "        # The first function is: h1 = relu(z*W1 + b1),  \n",
    "        h1 =  tf.nn.relu( tf.matmul(z,W1) + b1)\n",
    "\n",
    "        # The seconde one is: prob = h1*W2 + b2\n",
    "        prob = tf.matmul(h1,W2) + b2\n",
    "\n",
    "        # The last one is to use activation fuction sigmoid(prob)  \n",
    "        output = tf.nn.sigmoid(prob)\n",
    "        return output\n",
    "\n",
    "    def discriminator(self, img, W1, b1, W2, b2):\n",
    "        # TODO: discriminator is used to discriminate whether an input image is real or not.\n",
    "        \n",
    "        # The first function is: h1 = relu(img*W1 + b1)\n",
    "        h1 = tf.nn.relu( tf.matmul(img,W1) + b1)\n",
    "\n",
    "        # The second one is: logit = h1*W2 + b2\n",
    "        logit = tf.matmul(h1,W2) + b2\n",
    "\n",
    "        # The third one is to generate the probability of an input image to be a real one with sigmoid.\n",
    "        prob = tf.nn.sigmoid(logit)\n",
    "\n",
    "        return prob, logit\n",
    "\n",
    "    def plot(self, samples):\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(self.img_size, self.img_size), cmap=\"Greys_r\")\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def train(self, output_path):\n",
    "        d_optim = tf.train.AdamOptimizer().minimize(self.D_loss, var_list=self.D_vars)\n",
    "        g_optim = tf.train.AdamOptimizer().minimize(self.G_loss, var_list=self.G_vars)\n",
    "\n",
    "        try:\n",
    "            tf.global_variables_initializer().run()\n",
    "        except:\n",
    "            tf.initialize_all_variables().run()\n",
    "        \n",
    "        i = 0\n",
    "        counter = 1\n",
    "        mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)\n",
    "\n",
    "        for epoch in xrange(self.epoch):\n",
    "            input_z = np.random.uniform(-1, 1, [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "            input_imgs, _ = mnist.train.next_batch(self.batch_size) \n",
    "            \n",
    "            # Update D network\n",
    "            _, D_loss_curr = self.sess.run([d_optim, self.D_loss], feed_dict={self.img: input_imgs, self.z: input_z})\n",
    "            _, G_loss_curr = self.sess.run([g_optim, self.G_loss], feed_dict={self.z: input_z})\n",
    "            \n",
    "            counter += 1\n",
    "            if counter % 1000 == 0:\n",
    "                print(\"Epoch: [{:2d}] D_loss: {:.8f}, G_loss {:.8f}\".format(\n",
    "                    epoch, D_loss_curr, G_loss_curr))\n",
    "                \n",
    "                samples = self.sess.run(self.img_fake, feed_dict={self.z: input_z})\n",
    "                #import pdb; pdb.set_trace()\n",
    "                fig = self.plot(samples)\n",
    "                if not os.path.exists(output_path):\n",
    "                    os.makedirs(output_path)\n",
    "                plt.savefig(os.path.join(output_path, '{}.png'.format(str(i).zfill(3))), bbox_inches='tight')\n",
    "                i += 1\n",
    "                plt.close(fig)\n",
    "def run():\n",
    "    \n",
    "    flags = tf.app.flags\n",
    "    #flags.DEFINE_integer(\"img_size\", 28, \"Image size.\")\n",
    "    #flags.DEFINE_integer(\"z_dim\", 100, \"The dimension of random input z.\")\n",
    "    #flags.DEFINE_integer(\"hidden_dim\", 128, \"The dimension of hidden \")\n",
    "    #flags.DEFINE_integer(\"batch_size\", 16, \"The size of batch images [32]\")\n",
    "    #flags.DEFINE_integer(\"epoch\", 100000, \"Epoch to train!\")\n",
    "    FLAGS = flags.FLAGS\n",
    "\n",
    "    config = tf.ConfigProto( \n",
    "            device_count = {'GPU': 0}) # if you wanna implement this code with GPU, change 0 to 1 or 2.  \n",
    "    with tf.Session(config=config) as sess:\n",
    "        #vanilla_gan = VanillaGAN(sess, img_size=FLAGS.img_size, z_dim=FLAGS.z_dim, \n",
    "        #        hidden_dim=FLAGS.hidden_dim, batch_size=FLAGS.batch_size,\n",
    "        #        epoch=FLAGS.epoch)\n",
    "        vanilla_gan = VanillaGAN(sess, img_size=28, z_dim=100, hidden_dim=128, batch_size=16, epoch=100000)\n",
    "        vanilla_gan.train(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/ipykernel_launcher.py:\n",
      "  --batch_size: The size of batch images [32]\n",
      "    (default: '16')\n",
      "    (an integer)\n",
      "  --epoch: Epoch to train!\n",
      "    (default: '100000')\n",
      "    (an integer)\n",
      "  --hidden_dim: The dimension of hidden\n",
      "    (default: '128')\n",
      "    (an integer)\n",
      "  --img_size: Image size.\n",
      "    (default: '28')\n",
      "    (an integer)\n",
      "  --z_dim: The dimension of random input z.\n",
      "    (default: '100')\n",
      "    (an integer)\n",
      "\n",
      "absl.flags:\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\n",
      "    (default: '')\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
      "    the command line even if the program does not define a flag with that name.\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
      "    format.\n",
      "    (default: '')\n",
      "WARNING:tensorflow:From <ipython-input-20-abb8e63d1239>:131: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/adam/miniconda3/envs/keras_gpu_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: [998] D_loss: 0.00428481, G_loss 7.95637655\n",
      "Epoch: [1998] D_loss: 0.00719402, G_loss 7.60120726\n",
      "Epoch: [2998] D_loss: 0.00572491, G_loss 6.78329706\n",
      "Epoch: [3998] D_loss: 0.09468863, G_loss 4.05009127\n",
      "Epoch: [4998] D_loss: 0.34614816, G_loss 3.99129581\n",
      "Epoch: [5998] D_loss: 0.33901584, G_loss 3.36465883\n",
      "Epoch: [6998] D_loss: 0.40737125, G_loss 3.80433989\n",
      "Epoch: [7998] D_loss: 0.62274277, G_loss 3.51670361\n",
      "Epoch: [8998] D_loss: 0.46060354, G_loss 2.92376328\n",
      "Epoch: [9998] D_loss: 0.43678129, G_loss 3.82169747\n",
      "Epoch: [10998] D_loss: 1.31025350, G_loss 1.80455470\n",
      "Epoch: [11998] D_loss: 0.50747442, G_loss 2.58976698\n",
      "Epoch: [12998] D_loss: 0.35669684, G_loss 3.03784323\n",
      "Epoch: [13998] D_loss: 0.91276908, G_loss 2.23220539\n",
      "Epoch: [14998] D_loss: 0.75030309, G_loss 1.94497490\n",
      "Epoch: [15998] D_loss: 0.49856281, G_loss 2.66089535\n",
      "Epoch: [16998] D_loss: 0.62709761, G_loss 2.37704587\n",
      "Epoch: [17998] D_loss: 0.51973486, G_loss 2.61272192\n",
      "Epoch: [18998] D_loss: 0.87153679, G_loss 1.68011951\n",
      "Epoch: [19998] D_loss: 0.81107199, G_loss 1.52311230\n",
      "Epoch: [20998] D_loss: 0.70188487, G_loss 2.10898137\n",
      "Epoch: [21998] D_loss: 0.53869927, G_loss 1.89244592\n",
      "Epoch: [22998] D_loss: 0.68655992, G_loss 1.95809829\n",
      "Epoch: [23998] D_loss: 0.86483896, G_loss 2.06683016\n",
      "Epoch: [24998] D_loss: 1.31034684, G_loss 1.58223641\n",
      "Epoch: [25998] D_loss: 0.56066823, G_loss 2.06663561\n",
      "Epoch: [26998] D_loss: 0.73119152, G_loss 1.94123900\n",
      "Epoch: [27998] D_loss: 0.73467487, G_loss 1.60514998\n",
      "Epoch: [28998] D_loss: 0.88782352, G_loss 2.02891135\n",
      "Epoch: [29998] D_loss: 0.59513789, G_loss 2.14011955\n",
      "Epoch: [30998] D_loss: 0.69470000, G_loss 1.44023049\n",
      "Epoch: [31998] D_loss: 0.55695099, G_loss 1.98325348\n",
      "Epoch: [32998] D_loss: 1.20873618, G_loss 1.88610387\n",
      "Epoch: [33998] D_loss: 0.45710242, G_loss 1.71387279\n",
      "Epoch: [34998] D_loss: 0.73139441, G_loss 1.76739526\n",
      "Epoch: [35998] D_loss: 0.49724284, G_loss 2.31177568\n",
      "Epoch: [36998] D_loss: 0.63705826, G_loss 1.77913129\n",
      "Epoch: [37998] D_loss: 0.77953422, G_loss 2.11342645\n",
      "Epoch: [38998] D_loss: 0.72529125, G_loss 2.13349771\n",
      "Epoch: [39998] D_loss: 0.60220444, G_loss 2.21379232\n",
      "Epoch: [40998] D_loss: 0.45071220, G_loss 2.24446464\n",
      "Epoch: [41998] D_loss: 0.96979249, G_loss 1.54580283\n",
      "Epoch: [42998] D_loss: 0.48381644, G_loss 1.69023502\n",
      "Epoch: [43998] D_loss: 0.46075869, G_loss 2.29578257\n",
      "Epoch: [44998] D_loss: 0.85024691, G_loss 1.78710890\n",
      "Epoch: [45998] D_loss: 0.67916203, G_loss 1.77841616\n",
      "Epoch: [46998] D_loss: 0.64453024, G_loss 2.62414265\n",
      "Epoch: [47998] D_loss: 1.05959272, G_loss 1.44970298\n",
      "Epoch: [48998] D_loss: 0.73599851, G_loss 1.60525131\n",
      "Epoch: [49998] D_loss: 0.72508985, G_loss 1.84270191\n",
      "Epoch: [50998] D_loss: 0.48202997, G_loss 2.54228735\n",
      "Epoch: [51998] D_loss: 0.88925660, G_loss 1.43252003\n",
      "Epoch: [52998] D_loss: 0.51864207, G_loss 1.96188688\n",
      "Epoch: [53998] D_loss: 0.68655568, G_loss 2.12206173\n",
      "Epoch: [54998] D_loss: 0.67280376, G_loss 1.72225142\n",
      "Epoch: [55998] D_loss: 0.40114748, G_loss 2.38987613\n",
      "Epoch: [56998] D_loss: 1.00342655, G_loss 2.18447995\n",
      "Epoch: [57998] D_loss: 0.90370572, G_loss 1.72370577\n",
      "Epoch: [58998] D_loss: 0.52713877, G_loss 2.62316084\n",
      "Epoch: [59998] D_loss: 0.42177024, G_loss 2.36397743\n",
      "Epoch: [60998] D_loss: 0.67533469, G_loss 2.37272453\n",
      "Epoch: [61998] D_loss: 0.67380440, G_loss 2.27491808\n",
      "Epoch: [62998] D_loss: 0.64242244, G_loss 2.51122904\n",
      "Epoch: [63998] D_loss: 0.63274950, G_loss 2.05779266\n",
      "Epoch: [64998] D_loss: 0.66222161, G_loss 2.41798830\n",
      "Epoch: [65998] D_loss: 0.46317339, G_loss 2.57555270\n",
      "Epoch: [66998] D_loss: 0.60769343, G_loss 2.60536623\n",
      "Epoch: [67998] D_loss: 0.33631447, G_loss 2.27069926\n",
      "Epoch: [68998] D_loss: 0.34597948, G_loss 2.69267082\n",
      "Epoch: [69998] D_loss: 0.60070598, G_loss 2.46960020\n",
      "Epoch: [70998] D_loss: 0.56840265, G_loss 2.54181433\n",
      "Epoch: [71998] D_loss: 0.32556522, G_loss 2.35850954\n",
      "Epoch: [72998] D_loss: 0.33471179, G_loss 3.04217958\n",
      "Epoch: [73998] D_loss: 0.72096032, G_loss 1.96895289\n",
      "Epoch: [74998] D_loss: 0.49894941, G_loss 2.37589717\n",
      "Epoch: [75998] D_loss: 0.38251975, G_loss 2.33539820\n",
      "Epoch: [76998] D_loss: 0.46685433, G_loss 2.31872439\n",
      "Epoch: [77998] D_loss: 1.04734540, G_loss 2.28750706\n",
      "Epoch: [78998] D_loss: 0.45453894, G_loss 2.34644413\n",
      "Epoch: [79998] D_loss: 0.90216815, G_loss 1.62986660\n",
      "Epoch: [80998] D_loss: 0.51578850, G_loss 2.79222488\n",
      "Epoch: [81998] D_loss: 0.51581979, G_loss 2.17779684\n",
      "Epoch: [82998] D_loss: 0.46081394, G_loss 2.41233349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83998] D_loss: 1.37276077, G_loss 2.03716469\n",
      "Epoch: [84998] D_loss: 0.40168995, G_loss 2.90894079\n",
      "Epoch: [85998] D_loss: 0.68069530, G_loss 1.96116042\n",
      "Epoch: [86998] D_loss: 0.91392070, G_loss 1.45913649\n",
      "Epoch: [87998] D_loss: 0.56741202, G_loss 2.12412643\n",
      "Epoch: [88998] D_loss: 0.36690140, G_loss 2.30300164\n",
      "Epoch: [89998] D_loss: 0.52623254, G_loss 1.83894753\n",
      "Epoch: [90998] D_loss: 0.49283957, G_loss 2.89014530\n",
      "Epoch: [91998] D_loss: 0.15502262, G_loss 2.94583654\n",
      "Epoch: [92998] D_loss: 0.57611251, G_loss 2.67079449\n",
      "Epoch: [93998] D_loss: 0.37444341, G_loss 1.93436837\n",
      "Epoch: [94998] D_loss: 0.85130894, G_loss 2.87239337\n",
      "Epoch: [95998] D_loss: 0.65603876, G_loss 2.39556408\n",
      "Epoch: [96998] D_loss: 0.58951229, G_loss 2.25301123\n",
      "Epoch: [97998] D_loss: 0.71938479, G_loss 1.92636418\n",
      "Epoch: [98998] D_loss: 0.77530038, G_loss 2.38618302\n",
      "Epoch: [99998] D_loss: 0.52942622, G_loss 1.92087793\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_z = np.random.uniform(-1, 1, [9, 100]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vanilla_gan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d32fde84b6f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvanilla_gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vanilla_gan' is not defined"
     ]
    }
   ],
   "source": [
    "vanilla_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
